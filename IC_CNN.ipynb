{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IC CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBrlCDcryVe1"
      },
      "source": [
        "# Image Classification -- CNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iRRzLFbbPmm"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as k"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGoT0Ajld6lY"
      },
      "source": [
        "img_width, img_height = 224, 224\n",
        "\n",
        "train_data_dir = '/content/drive/My Drive/Image Classifier Data/train'\n",
        "validation_data_dir = '/content/drive/My Drive/Image Classifier Data/test'\n",
        "\n",
        "nb_train_samples = 400\n",
        "nb_validation_samples = 100\n",
        "epochs = 10\n",
        "batch_size = 16"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBW_TlNBe8lg"
      },
      "source": [
        "if k.image_data_format() == 'channels first':\n",
        "  input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "  input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soIfWQnPkWHg"
      },
      "source": [
        "####Defining the layers of our CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn9fwkhhs7EC"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (2,2), input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size= (2,2)))\n",
        "\n",
        "model.add(Conv2D(32, (2,2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size= (2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (2,2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size= (2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dC4AHLJwfyO"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16LlIf-PpcZp"
      },
      "source": [
        "###Training Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdsRdvitxIip",
        "outputId": "57adf0a3-7996-49af-af77-34d6ae95e672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1. /255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. /255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_data_dir, \n",
        "                                                    target_size=(img_width, img_height), \n",
        "                                                    batch_size=batch_size, \n",
        "                                                    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(validation_data_dir, \n",
        "                                                        target_size=(img_width, img_height), \n",
        "                                                        batch_size=batch_size, \n",
        "                                                        class_mode='binary')\n",
        "\n",
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch = nb_train_samples // batch_size, \n",
        "                    epochs = epochs, \n",
        "                    validation_data = validation_generator, \n",
        "                    validation_steps = nb_validation_samples // batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "Found 100 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From <ipython-input-6-e673294c2542>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 354s 14s/step - loss: 0.8783 - accuracy: 0.6425 - val_loss: 0.3930 - val_accuracy: 0.8854\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 6s 235ms/step - loss: 0.5174 - accuracy: 0.7575 - val_loss: 0.4462 - val_accuracy: 0.7604\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 6s 236ms/step - loss: 0.3980 - accuracy: 0.8275 - val_loss: 0.4136 - val_accuracy: 0.8021\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 6s 235ms/step - loss: 0.3767 - accuracy: 0.8250 - val_loss: 0.2843 - val_accuracy: 0.9062\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 6s 236ms/step - loss: 0.3346 - accuracy: 0.8600 - val_loss: 0.3287 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 6s 235ms/step - loss: 0.3346 - accuracy: 0.8750 - val_loss: 0.3574 - val_accuracy: 0.8542\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 6s 233ms/step - loss: 0.2814 - accuracy: 0.8925 - val_loss: 0.4132 - val_accuracy: 0.7917\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 6s 235ms/step - loss: 0.3168 - accuracy: 0.8675 - val_loss: 0.4531 - val_accuracy: 0.7917\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 0.2589 - accuracy: 0.9050 - val_loss: 0.3596 - val_accuracy: 0.8229\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 6s 234ms/step - loss: 0.3047 - accuracy: 0.8825 - val_loss: 0.2571 - val_accuracy: 0.9062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbac0193518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaG_MFJCjhqP"
      },
      "source": [
        "###Testing our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJJbcecdWqcb",
        "outputId": "fac67b35-c6a0-4d45-c0ff-b729508e8b6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "\n",
        "test_image = image.load_img('/content/carr.png', target_size = (224, 224))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = model.predict(test_image)\n",
        "\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'Aeroplane'\n",
        "else:\n",
        "    prediction = 'Car'\n",
        "print(prediction)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IicDnT0jZWp"
      },
      "source": [
        "###Creating a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ougyOHTubjnw"
      },
      "source": [
        "def checkoutput(testimg):\n",
        "\n",
        "  test_image = image.load_img(testimg, target_size = (224, 224))\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  result = model.predict(test_image)\n",
        "\n",
        "  if result[0][0] == 1:\n",
        "      prediction = 'Aeroplane'\n",
        "  else:\n",
        "      prediction = 'Car'\n",
        "\n",
        "  print('Result:',prediction)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9rUgwTOiYoh",
        "outputId": "f7912b4a-7638-42b0-a11c-d792537096b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "checkoutput('/content/planee.jpg')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result: Aeroplane\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}